# -*- coding: utf-8 -*-
"""Auto_sector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eduI5O_8Lp9Bgoo26EoX7dDL10nSYLiq
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import files
data=files.upload()

"""#use_case_1"""

df_dealers = pd.read_excel('Dataset.xlsx',sheet_name='Dealers')

df_customers = pd.read_excel('Dataset.xlsx',sheet_name='Customers')

df_claims = pd.read_excel('Dataset.xlsx',sheet_name='Claims')

df_parts = pd.read_excel('Dataset.xlsx',sheet_name='Parts')

df_transactions = pd.read_excel('Dataset.xlsx',sheet_name='Transactions')

df_vendors = pd.read_excel('Dataset.xlsx',sheet_name='Vendors')

df_dealers.head()

df_customers.head()

df_claims.head()

df_parts.head()

df_transactions.head()

df_vendors.head()

january_data = df_claims[df_claims['claim_date'].dt.month == 1]
january_data['Part_ID'].value_counts().head(10)

df_claims['Dealer_ID'].value_counts().head(10)
plt.subplots(figsize=(12,4))
sns.countplot(x='Dealer_ID',order=pd.value_counts(df_claims['Dealer_ID']).iloc[:10].index,data=df_claims)

failed_dealers=df_claims.groupby(['Dealer_ID']).count()
df_failed_dealers=failed_dealers.sort_values(by='Cust_ID',ascending=False).reset_index().head(10)['Dealer_ID']

df_failed_dealers

failed_dealers_info=pd.DataFrame()
for i in df_failed_dealers:
  failed_dealers_info=pd.concat([failed_dealers_info,df_dealers[df_dealers['Dealer_ID']==i]])

failed_dealers_info

df_claims['Repair_or_Replace'].value_counts().head(10)

"""#Use case_3"""

df_claims.head()

df_claims['Part_ID'].unique()

Repair_or_Replace=pd.get_dummies(df_claims['Repair_or_Replace'],drop_first=True)

df_claims.drop(['Repair_or_Replace'],axis=1,inplace=True)

df_claims = pd.concat([df_claims,Repair_or_Replace],axis=1)

df_claims.head()

df_claims.corr()*100

df_claims.columns

df_claims.columns = ['claim_id', 'claim_date', 'claim_amount', 'Dealer_ID', 'Cust_ID',
       'Part_ID', 'Target_column']

df_claims.head()

df_claims.drop(['claim_date'],axis=1,inplace=True)

df_claims.head()

df_claims.drop(['Dealer_ID','Cust_ID'],axis=1,inplace=True)

df_claims['claim_id'].nunique()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(df_claims.drop('Target_column',axis=1))

scaled_features = scaler.transform(df_claims.drop('Target_column',axis=1))

df_feat = pd.DataFrame(scaled_features,columns=['claim_id', 'claim_amount','Part_ID'])
df_feat.head()

sns.heatmap(df_claims[['claim_id', 'claim_amount','Part_ID']].corr()*100,annot=True)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_feat,df_claims['Target_column'],
                                                    test_size=0.20,random_state=50)

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)

cm

y_test.shape

accuracy_score(y_test,y_pred)*100

"""#use case_2"""

failed_parts=df_parts.groupby(['Part_ID']).count()
df_failed_parts=failed_parts.sort_values(by='Part_ID',ascending=False).reset_index().head(10)['Part_ID']

df_failed_parts

failed_parts_info=pd.DataFrame()
for i in df_failed_parts:
  failed_parts_info=pd.concat([failed_parts_info,df_parts[df_parts['Part_ID']==i]])

failed_parts_info

from numpy.ma.core import append
age=[]
for i in range (len(df_claims)):
  for j in range(len(df_parts)):
    if df_claims.loc[i]['Part_ID']==df_parts.loc[j]["Part_ID"]:
      age.append((df_claims.loc[i]["claim_date"]-df_parts.loc[j]["Manf_Date"])/np.timedelta64(1,'Y'))

age = np.array(age)

milage=age*12000

milage=np.array(milage)

df_claims['age']=age

df_claims['milage']=milage

new_dataframe=df_claims.merge(df_dealers)

new_dataframe.head(5)

new_dataframe=new_dataframe.round({"age":0,"milage":0})

new_dataframe['City']=pd.factorize(new_dataframe['City'])[0]

new_dataframe.tail(5)

again_new=new_dataframe[['claim_id','age','milage','City','Part_ID']]

again_new.head(5)

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
A=again_new.drop('Part_ID',axis=1)

scaler.fit(A)

A

scaled_features=scaler.transform(A)

df_feat = pd.DataFrame(scaled_features,columns=['claim_id', 'age','milage','City'])
df_feat.head()

sns.heatmap(A[['claim_id', 'age','milage','City']].corr()*100,annot=True)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_feat,again_new['Part_ID'],
                                                    test_size=0.20,random_state=50)

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test, y_pred)

cm

y_test.shape

accuracy_score(y_test,y_pred)*100

"""#use_case_4"""

# Import NLP libraries
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

data = files.upload()

kindle_review = pd.read_csv('preprocessed_kindle_review .csv', index_col=0)

kindle_review.drop('summary', axis = 1, inplace=True)

kindle_review.head()

kindle_review.info()

cust_names = list(df_customers['Cust_Name'].unique())

cust_ret = df_claims.merge(df_customers)

cust_ret.head()

df_customers.nunique()

cust = list(cust_ret['Cust_Name'].unique())

cust_ret.nunique()

retention_cx = [customer for customer in cust_names if customer not in cust]

len(retention_cx)

retention = pd.DataFrame(columns=df_customers.columns)
for i in retention_cx:
  if i not in retention['Cust_Name']:
    retention = pd.concat([retention,df_customers[df_customers['Cust_Name']== i]])

retention = retention.drop('Cust_ID', axis=1).drop_duplicates()

retention.head()

retention.to_excel('retention Customers.xlsx')

files.download('retention Customers.xlsx')

# Valence Aware Dictionary and sEntiment Reasoner
nltk.download(['averaged_perceptron_tagger', 'punkt', 'vader_lexicon'])

sia = SentimentIntensityAnalyzer()
scores = []
for i in range(len(kindle_review)):
  text = kindle_review['reviewText'][i]
  score = sia.polarity_scores(text)
  scores.append(score['compound'])

scores = np.array(scores)

kindle_review['Positive_Scores'] = scores

kindle_review.head()

df_customers.info()

customers_unique = df_customers.drop('Cust_ID',axis=1).drop_duplicates()

customers_unique.info()

customers_unique.reset_index(inplace = True)

customers_reviews = pd.concat([customers_unique, kindle_review.iloc[:1970]], axis=1)

customers_reviews

customers_reviews['Positive_Scores'].mean()

retention_customers = customers_reviews[customers_reviews['Positive_Scores'] < 0.48810253807106596]

retention_customers.head()

retention_customers.info()

